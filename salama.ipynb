{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"how are this\",\n",
    "    \"how are you\",\n",
    "    \"how are you doing\",\n",
    "    \"what is your name\",\n",
    "    \"tell me about yourself\",\n",
    "    \"yet another question\",\n",
    "    \"what is your age\",\n",
    "    \"last question\",\n",
    "    \"please answer me\",\n",
    "    \"key word\",\n",
    "    \"one more question\",\n",
    "    \"for the last time\",\n",
    "    \"please please answer me\",\n",
    "    \"you look good\",\n",
    "    \"you look great\",\n",
    "    \"this project is great\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the csv file dataset/train.csv\n",
    "file_data = pd.read_csv(\"./Dataset/Dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file_data['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x31 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = None\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\S+')\n",
    "vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x31 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 52 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_IDF = None\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TF_IDF = TfidfVectorizer(token_pattern=r'\\S+')\n",
    "TF_IDF.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vectorizer.transform(data)\n",
    "tf_idf = TF_IDF.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words.toarray())\n",
    "print(len(bag_of_words.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.55478891 0.\n",
      "  0.         0.         0.         0.55478891 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.62001494\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.59486632 0.\n",
      "  0.         0.         0.         0.59486632 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54061827 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.47283995 0.60678295\n",
      "  0.         0.         0.         0.47283995 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42971993 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.56576838 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.52651698 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.41029211 0.         0.         0.\n",
      "  0.         0.         0.         0.52651698 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52651698]\n",
      " [0.         0.         0.6193118  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.48260314 0.         0.         0.\n",
      "  0.         0.         0.         0.6193118  0.         0.\n",
      "  0.        ]\n",
      " [0.         0.56576838 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.7452182  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.66682069 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.59754328 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.53468128 0.         0.         0.\n",
      "  0.59754328 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.70710678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.6193118  0.         0.6193118\n",
      "  0.         0.         0.48260314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.51581894 0.         0.         0.         0.         0.\n",
      "  0.44921315 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.51581894 0.\n",
      "  0.51581894 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.41520357 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37152385 0.         0.         0.\n",
      "  0.83040714 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66519607 0.         0.         0.         0.\n",
      "  0.         0.57930176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.47108774 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.61299038 0.         0.         0.\n",
      "  0.         0.61299038 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.49848329 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4927128  0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56576838 0.         0.         0.         0.4927128\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf.toarray())\n",
    "print(len(tf_idf.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"how are you\",\n",
    "    \"what is your name\",\n",
    "    \"do you know me\",\n",
    "    \"your work is so good\",\n",
    "    \"you look great\",\n",
    "    \"this project failed\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = pd.read_csv(\"./Dataset/Dataset/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data_file['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any word in test not in vectorizer vocabulary\n",
    "#test_data_preprocessed = [' '.join([word for word in sentence.split() if word in vectorizer.vocabulary_]) for sentence in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bag_of_words = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_idf = TF_IDF.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(test_bag_of_words.toarray())\n",
    "print(len(test_bag_of_words.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.59486632 0.\n",
      "  0.         0.         0.         0.59486632 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54061827 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.56576838 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.74004489 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.67255747 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.650165   0.         0.         0.50664572 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.56621161\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.61299038 0.         0.         0.\n",
      "  0.         0.61299038 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.49848329 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75411672 0.         0.         0.         0.65674042\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(test_tf_idf.toarray())\n",
    "print(len(test_tf_idf.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm\n",
    "# concatenate the two features\n",
    "#bag_of_words = bag_of_words.toarray()\n",
    "clf = svm.SVC()\n",
    "clf.fit(bag_of_words, file_data['stance'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predited = clf.predict(test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = test_data_file['stance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38719891628832004\n"
     ]
    }
   ],
   "source": [
    "# f1 score\n",
    "\n",
    "stance_f1 = f1_score(actual, predited, average='macro')\n",
    "print(stance_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another classifier for category\n",
    "clf2 = svm.SVC()\n",
    "clf2.fit(bag_of_words, file_data['category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predited2 = clf2.predict(test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual2 = test_data_file['category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23648439875233446\n"
     ]
    }
   ],
   "source": [
    "category_f1 = f1_score(actual2, predited2, average='macro')\n",
    "print(category_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro f1 score for both\n",
    "macro_f1 = (stance_f1 + category_f1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3118416575203272\n"
     ]
    }
   ],
   "source": [
    "print(macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(bag_of_words, file_data['stance'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predited = nb.predict(test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41022801278825377\n"
     ]
    }
   ],
   "source": [
    "nb_stance_f1 = f1_score(actual, nb_predited, average='macro')\n",
    "print(nb_stance_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2 = naive_bayes.MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb2.fit(bag_of_words, file_data['category'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2_predited = nb2.predict(test_bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2505719059915458\n"
     ]
    }
   ],
   "source": [
    "nb_category_f1 = f1_score(actual2, nb2_predited, average='macro')\n",
    "print(nb_category_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro f1 score for both\n",
    "nb_macro_f1 = (nb_stance_f1 + nb_category_f1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33039995938989974\n"
     ]
    }
   ],
   "source": [
    "print(nb_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.07      0.13        70\n",
      "           0       0.50      0.08      0.14       126\n",
      "           1       0.82      0.99      0.90       804\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.63      0.38      0.39      1000\n",
      "weighted avg       0.76      0.81      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use sklearn metrics to get the confusion matrix\n",
    "print(metrics.classification_report(actual, predited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      advice       0.00      0.00      0.00        10\n",
      "   celebrity       0.87      0.71      0.78       145\n",
      "   info_news       0.65      0.95      0.77       545\n",
      "      others       0.00      0.00      0.00        17\n",
      "    personal       0.68      0.36      0.47       128\n",
      "        plan       1.00      0.01      0.02        82\n",
      "    requests       1.00      0.05      0.10        20\n",
      "restrictions       0.00      0.00      0.00         2\n",
      "      rumors       0.00      0.00      0.00        15\n",
      "   unrelated       0.62      0.14      0.23        36\n",
      "\n",
      "    accuracy                           0.67      1000\n",
      "   macro avg       0.48      0.22      0.24      1000\n",
      "weighted avg       0.69      0.67      0.60      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actual2, predited2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.04      0.08        70\n",
      "           0       0.53      0.17      0.25       126\n",
      "           1       0.83      0.98      0.90       804\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.70      0.40      0.41      1000\n",
      "weighted avg       0.78      0.81      0.76      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actual, nb_predited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      advice       0.00      0.00      0.00        10\n",
      "   celebrity       0.92      0.77      0.83       145\n",
      "   info_news       0.68      0.94      0.79       545\n",
      "      others       0.00      0.00      0.00        17\n",
      "    personal       0.59      0.49      0.54       128\n",
      "        plan       0.00      0.00      0.00        82\n",
      "    requests       0.00      0.00      0.00        20\n",
      "restrictions       0.00      0.00      0.00         2\n",
      "      rumors       0.00      0.00      0.00        15\n",
      "   unrelated       0.80      0.22      0.35        36\n",
      "\n",
      "    accuracy                           0.70      1000\n",
      "   macro avg       0.30      0.24      0.25      1000\n",
      "weighted avg       0.61      0.70      0.63      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(actual2, nb2_predited))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement TF-IDF\n",
    "data = [\n",
    "    \"how are this\",\n",
    "    \"how are you\",\n",
    "    \"how are you doing\",\n",
    "    \"what is your name\",\n",
    "    \"tell me about yourself\",\n",
    "    \"yet another question\",\n",
    "    \"what is your age\",\n",
    "    \"last question\",\n",
    "    \"please answer me\",\n",
    "    \"key word\",\n",
    "    \"one more question\",\n",
    "    \"for the last time\",\n",
    "    \"please please answer me\",\n",
    "    \"you look good\",\n",
    "    \"you look great\",\n",
    "    \"this project is great\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = None\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TF_IDF = TfidfVectorizer(token_pattern=r'\\S+')\n",
    "TF_IDF.fit_transform(data)\n",
    "\n",
    "tf_idf_bag_of_words = TF_IDF.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.55478891 0.\n",
      "  0.         0.         0.         0.55478891 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.62001494\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.59486632 0.\n",
      "  0.         0.         0.         0.59486632 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54061827 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.47283995 0.60678295\n",
      "  0.         0.         0.         0.47283995 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42971993 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.56576838 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.52651698 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.41029211 0.         0.         0.\n",
      "  0.         0.         0.         0.52651698 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52651698]\n",
      " [0.         0.         0.6193118  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.48260314 0.         0.         0.\n",
      "  0.         0.         0.         0.6193118  0.         0.\n",
      "  0.        ]\n",
      " [0.         0.56576838 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.7452182  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.66682069 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.59754328 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.53468128 0.         0.         0.\n",
      "  0.59754328 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.70710678\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.6193118  0.         0.6193118\n",
      "  0.         0.         0.48260314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.51581894 0.         0.         0.         0.         0.\n",
      "  0.44921315 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.51581894 0.\n",
      "  0.51581894 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.41520357 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37152385 0.         0.         0.\n",
      "  0.83040714 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.66519607 0.         0.         0.         0.\n",
      "  0.         0.57930176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.47108774 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.61299038 0.         0.         0.\n",
      "  0.         0.61299038 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.49848329 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4927128  0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.56576838 0.         0.         0.         0.4927128\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_idf_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    \"how are you\",\n",
    "    \"what is your name\",\n",
    "    \"do you know me\",\n",
    "    \"your work is so good\",\n",
    "    \"you look great\",\n",
    "    \"this project failed\"\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_idf_bag_of_words = TF_IDF.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.59486632 0.\n",
      "  0.         0.         0.         0.59486632 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.54061827 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.44087905 0.\n",
      "  0.         0.         0.         0.         0.56576838 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4927128  0.         0.         0.         0.4927128\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.74004489 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.67255747 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.650165   0.         0.         0.50664572 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.56621161\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.61299038 0.         0.         0.\n",
      "  0.         0.61299038 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.49848329 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75411672 0.         0.         0.         0.65674042\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(test_tf_idf_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
