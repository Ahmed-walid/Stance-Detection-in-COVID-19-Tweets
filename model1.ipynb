{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tashaphyne\n",
      "  Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
      "Collecting pyarabic\n",
      "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\moham\\appdata\\roaming\\python\\python39\\site-packages (from pyarabic->tashaphyne) (1.16.0)\n",
      "Installing collected packages: pyarabic, tashaphyne\n",
      "Successfully installed pyarabic-0.6.15 tashaphyne-0.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarabic in c:\\python39\\lib\\site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\moham\\appdata\\roaming\\python\\python39\\site-packages (from pyarabic) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Using legacy 'setup.py install' for emoji, since package 'wheel' is not installed.\n",
      "Installing collected packages: emoji\n",
      "    Running setup.py install for emoji: started\n",
      "    Running setup.py install for emoji: finished with status 'done'\n",
      "Successfully installed emoji-2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Python39\\python.exe -m pip install --upgrade pip' command.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string                             \n",
    "\n",
    "!pip install tashaphyne\n",
    "import tashaphyne\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "!pip install pyarabic\n",
    "import pyarabic\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "import nltk                             \n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')                                \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, encoding='UTF-8')\n",
    "    return  np.asarray(df['text'].values), np.asarray(df['category'].values), np.asarray(df['stance'].values)\n",
    "\n",
    "train_tweets, train_categories, train_stances = load_data('Dataset/train.csv')\n",
    "dev_tweets, dev_categories, dev_stances = load_data('Dataset/dev.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    ArListem = ArabicLightStemmer()\n",
    "\n",
    "    def stem_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            ArListem.light_stem(tweet_tokens[i])\n",
    "            tweet_tokens[i] = ArListem.get_root()\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_url(tweet):\n",
    "        tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_retweet_tag(tweet):\n",
    "        tweet = re.sub(r'^RT[\\s]+', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_tweet_mentions(tweet):\n",
    "        tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_text_control_tags(tweet):\n",
    "        tweet = re.sub(r'\\n|\\t|\\r|<LF>|<lf>', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def tokenize_tweet(tweet):\n",
    "        tweet_tokens = re.split(r',|،|_|-|!| ', tweet)\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_stopwords_punctuation(tweet_tokens, stop_words = ()):\n",
    "        tweet_reduced = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stop_words and word not in string.punctuation):\n",
    "                tweet_reduced.append(word)\n",
    "                tweet_reduced[-1] = re.sub(r'[~`!@#$%^&*()-/_+={}[\\]|/\\:;\"`<>,.?؟،]+', ' ', tweet_reduced[-1])\n",
    "        return tweet_reduced\n",
    "\n",
    "    def remove_specialcharacters(tweet):\n",
    "        tweet = re.sub(r'#', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def handle_emojis(tweet, remove_emojis = True):\n",
    "        if not remove_emojis:\n",
    "            tweet = emoji.demojize(tweet, language='en') # convert emojis to text ENGLISH!!\n",
    "        else:\n",
    "            for e in tweet:\n",
    "                if emoji.is_emoji(e):\n",
    "                   tweet = tweet.replace(e, '')\n",
    "        return tweet\n",
    "\n",
    "    def normalize_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tashkeel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tatweel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_lastharaka(tweet_tokens[i])\n",
    "            tweet_tokens[i] = re.sub(r'(.)\\1{3,}', r\"\\1\\1\\1\", tweet_tokens[i]) # Remove longation\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_alef(tweet_tokens[i])\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_hamza(tweet_tokens[i])\n",
    "        return tweet_tokens\n",
    "\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_retweet_tag(tweet)\n",
    "    tweet = remove_specialcharacters(tweet)\n",
    "    tweet = remove_text_control_tags(tweet)\n",
    "    tweet = remove_tweet_mentions(tweet)\n",
    "    tweet = handle_emojis(tweet, remove_emojis = True)\n",
    "    tweet_tokens = tokenize_tweet(tweet)\n",
    "    # tweet_tokens = normalize_tweet(tweet_tokens)\n",
    "    tweet_tokens = remove_stopwords_punctuation(tweet_tokens, stopwords.words('arabic'))\n",
    "    # tweet_tokens = stem_tweet(tweet_tokens)\n",
    "    tweet = ' '.join(tweet_tokens)\n",
    "    tweet = re.sub(r'[^\\w\\s]+|\\d+',' ',tweet)  # to remove non-printable characters and numbers\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)     # to remove multiple spaces\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in train_tweets:\n",
    "    print(preprocess_tweet(tweet))\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\S+')\n",
    "vectorizer.fit_transform(train_tweets)\n",
    "def extract_bags_of_words(data): \n",
    "    bow = vectorizer.transform(data)\n",
    "    return bow.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = TfidfVectorizer(token_pattern=r'\\S+')\n",
    "TF_IDF.fit_transform(train_tweets)\n",
    "def extract_tf_idf(data):\n",
    "    tf_idf = TF_IDF.transform(data)\n",
    "    return tf_idf.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = extract_bags_of_words(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating TF-IDF of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = extract_tf_idf(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = extract_bags_of_words(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the TF-IDF for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_idf = extract_tf_idf(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = np.concatenate((train_bow, train_tf_idf), axis=1)\n",
    "train_features = train_bow\n",
    "# test_features = np.concatenate((test_bow, test_tf_idf), axis=1)\n",
    "test_features = test_bow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(train_features, train_stances.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = naive_bayes.MultinomialNB(alpha=0.13)\n",
    "nb_clf.fit(train_features[:4000], train_stances[:4000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(dev_stances, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions_f1 = f1_score(dev_stances, svm_predictions, average='macro')\n",
    "print(svm_predictions_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(dev_stances, nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions_f1 = f1_score(dev_stances, nb_predictions, average='macro')\n",
    "print(nb_predictions_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
