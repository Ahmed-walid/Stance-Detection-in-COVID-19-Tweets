{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string                             \n",
    "\n",
    "!pip install tashaphyne\n",
    "import tashaphyne\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "!pip install pyarabic\n",
    "import pyarabic\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "import nltk                             \n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')                                \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path, encoding='UTF-8')\n",
    "    return  np.asarray(df['text'].values), np.asarray(df['category'].values), np.asarray(df['stance'].values)\n",
    "\n",
    "train_tweets, train_categories, train_stances = load_data('Dataset/train.csv')\n",
    "dev_tweets, dev_categories, dev_stances = load_data('Dataset/dev.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    ArListem = ArabicLightStemmer()\n",
    "\n",
    "    def stem_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            ArListem.light_stem(tweet_tokens[i])\n",
    "            tweet_tokens[i] = ArListem.get_root()\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_url(tweet):\n",
    "        tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_retweet_tag(tweet):\n",
    "        tweet = re.sub(r'^RT[\\s]+', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_tweet_mentions(tweet):\n",
    "        tweet = re.sub(r'@\\w+', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_text_control_tags(tweet):\n",
    "        tweet = re.sub(r'\\n|\\t|\\r|<LF>|<lf>', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def tokenize_tweet(tweet):\n",
    "        tweet_tokens = re.split(r',|،|_|-|!| ', tweet)\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_stopwords_punctuation(tweet_tokens, stop_words = ()):\n",
    "        tweet_reduced = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stop_words and word not in string.punctuation):\n",
    "                tweet_reduced.append(word)\n",
    "                tweet_reduced[-1] = re.sub(r'[~`!@#$%^&*()-/_+={}[\\]|/\\:;\"`<>,.?؟،]+', ' ', tweet_reduced[-1])\n",
    "        return tweet_reduced\n",
    "\n",
    "    def remove_specialcharacters(tweet):\n",
    "        tweet = re.sub(r'#', ' ', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def handle_emojis(tweet, remove_emojis = True):\n",
    "        if not remove_emojis:\n",
    "            tweet = emoji.demojize(tweet, language='en') # convert emojis to text ENGLISH!!\n",
    "        else:\n",
    "            for e in tweet:\n",
    "                if emoji.is_emoji(e):\n",
    "                   tweet = tweet.replace(e, '')\n",
    "        return tweet\n",
    "\n",
    "    def normalize_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tashkeel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tatweel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_lastharaka(tweet_tokens[i])\n",
    "            tweet_tokens[i] = re.sub(r'(.)\\1{3,}', r\"\\1\\1\\1\", tweet_tokens[i]) # Remove longation\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_alef(tweet_tokens[i])\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_hamza(tweet_tokens[i])\n",
    "        return tweet_tokens\n",
    "\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_retweet_tag(tweet)\n",
    "    tweet = remove_specialcharacters(tweet)\n",
    "    tweet = remove_text_control_tags(tweet)\n",
    "    tweet = remove_tweet_mentions(tweet)\n",
    "    tweet = handle_emojis(tweet, remove_emojis = True)\n",
    "    tweet_tokens = tokenize_tweet(tweet)\n",
    "    # tweet_tokens = normalize_tweet(tweet_tokens)\n",
    "    tweet_tokens = remove_stopwords_punctuation(tweet_tokens, stopwords.words('arabic'))\n",
    "    # tweet_tokens = stem_tweet(tweet_tokens)\n",
    "    tweet = ' '.join(tweet_tokens)\n",
    "    tweet = re.sub(r'[^\\w\\s]+|\\d+',' ',tweet)  # to remove non-printable characters and numbers\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)     # to remove multiple spaces\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in train_tweets:\n",
    "    print(preprocess_tweet(tweet))\n",
    "    print('-----------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\S+')\n",
    "vectorizer.fit_transform(train_tweets)\n",
    "def extract_bags_of_words(data): \n",
    "    bow = vectorizer.transform(data)\n",
    "    return bow.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = TfidfVectorizer(token_pattern=r'\\S+')\n",
    "TF_IDF.fit_transform(train_tweets)\n",
    "def extract_tf_idf(data):\n",
    "    tf_idf = TF_IDF.transform(data)\n",
    "    return tf_idf.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = extract_bags_of_words(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = extract_tf_idf(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = extract_bags_of_words(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the TF-IDF for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_idf = extract_tf_idf(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = np.concatenate((train_bow, train_tf_idf), axis=1)\n",
    "train_features = train_bow\n",
    "# test_features = np.concatenate((test_bow, test_tf_idf), axis=1)\n",
    "test_features = test_bow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(train_features, train_stances.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = naive_bayes.MultinomialNB(alpha=0.13)\n",
    "nb_clf.fit(train_features[:4000], train_stances[:4000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(dev_stances, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions_f1 = f1_score(dev_stances, svm_predictions, average='macro')\n",
    "print(svm_predictions_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(dev_stances, nb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions_f1 = f1_score(dev_stances, nb_predictions, average='macro')\n",
    "print(nb_predictions_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
