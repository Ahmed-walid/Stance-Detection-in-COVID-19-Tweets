{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tashaphyne in /home/ahmed/.local/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: pyarabic in /home/ahmed/.local/lib/python3.10/site-packages (from tashaphyne) (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from pyarabic->tashaphyne) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarabic in /home/ahmed/.local/lib/python3.10/site-packages (0.6.15)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from pyarabic) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /home/ahmed/.local/lib/python3.10/site-packages (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ahmed/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string                             \n",
    "\n",
    "!pip install tashaphyne\n",
    "import tashaphyne\n",
    "from tashaphyne.stemming import ArabicLightStemmer\n",
    "\n",
    "!pip install pyarabic\n",
    "import pyarabic\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "import nltk                             \n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')                                \n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "df = pd.read_csv('Dataset/train.csv', encoding='UTF-8')\n",
    "train_tweets = df['text']\n",
    "train_categories = df['category']\n",
    "train_stances = df['stance']\n",
    "\n",
    "df = pd.read_csv('Dataset/dev.csv', encoding='UTF-8')\n",
    "dev_tweets = df['text']\n",
    "dev_categories = df['category']\n",
    "dev_stances = df['stance']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    ArListem = ArabicLightStemmer()\n",
    "\n",
    "    def stem_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            ArListem.light_stem(tweet_tokens[i])\n",
    "            tweet_tokens[i] = ArListem.get_root()\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_url(tweet):\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_retweet_tag(tweet):\n",
    "        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_tweet_mentions(tweet):\n",
    "        tweet = re.sub(r'@\\w+', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def remove_text_control_tags(tweet):\n",
    "        tweet = re.sub(r'\\n|\\t|\\r|<LF>|<lf>', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def tokenize_tweet(tweet):\n",
    "        tweet_tokens = re.split(', |_|-|!', tweet)\n",
    "        return tweet_tokens\n",
    "\n",
    "    def remove_stopwords_punctuation(tweet_tokens, stop_words = ()):\n",
    "        tweet_reduced = []\n",
    "        for word in tweet_tokens:\n",
    "            if (word not in stop_words and word not in string.punctuation):\n",
    "                tweet_reduced.append(word)\n",
    "                tweet_reduced[-1] = re.sub(r'[~`!@#$%^&*()-_+={}[\\]|/\\:;\"`<>,.?؟،]+', '', tweet_reduced[-1])\n",
    "        return tweet_reduced\n",
    "\n",
    "    def remove_specialcharacters(tweet):\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        return tweet\n",
    "\n",
    "    def handle_emojis(tweet):\n",
    "        tweet = emoji.demojize(tweet, language='en') # convert emojis to text ENGLISH!!\n",
    "        return tweet\n",
    "\n",
    "    def normalize_tweet(tweet_tokens):\n",
    "        for i in range(len(tweet_tokens)):\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tashkeel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_tatweel(tweet_tokens[i])\n",
    "            tweet_tokens[i] = pyarabic.araby.strip_lastharaka(tweet_tokens[i])\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_alef(tweet_tokens[i])\n",
    "            tweet_tokens[i] = re.sub(r'(.)\\1{3,}', r\"\\1\\1\\1\", tweet_tokens[i]) # Remove longation\n",
    "            # tweet_tokens[i] = pyarabic.araby.normalize_hamza(tweet_tokens[i])\n",
    "        return tweet_tokens\n",
    "\n",
    "    tweet = remove_url(tweet)\n",
    "    tweet = remove_retweet_tag(tweet)\n",
    "    tweet = remove_tweet_mentions(tweet)\n",
    "    tweet = remove_specialcharacters(tweet)\n",
    "    tweet = remove_text_control_tags(tweet)\n",
    "    tweet = handle_emojis(tweet)\n",
    "    tweet_tokens = tokenize_tweet(tweet)\n",
    "    tweet_tokens = normalize_tweet(tweet_tokens)\n",
    "    tweet_tokens_reduced = remove_stopwords_punctuation(tweet_tokens, stopwords.words('arabic'))\n",
    "    # tweet_tokens_stemmed = stem_tweet(tweet_tokens_reduced)\n",
    "    tweet = ' '.join(tweet_tokens_reduced)\n",
    "    return re.sub(r'[^\\w\\s]+','',tweet)  # to remove non-printable characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بيل غيتس يتلقى لقاح كوفيد من غير تصوير الابرة و لا السيرنجة و لا الدواء و لابس بولو صيفي في عز الشتاء و يقول ان إحدى مزايا عمر ال  عاما هي انه مؤهل للحصول على اللقاح  يعنى ما كان يحتاج اللقاح لو كان عمره اصغر من  thinking face \n"
     ]
    }
   ],
   "source": [
    "print(preprocess_tweet(train_tweets[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\S+')\n",
    "vectorizer.fit_transform(train_tweets)\n",
    "def extract_bags_of_words(data): \n",
    "    bow = vectorizer.transform(data)\n",
    "    return bow.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = TfidfVectorizer(token_pattern=r'\\S+')\n",
    "TF_IDF.fit_transform(train_tweets)\n",
    "def extract_tf_idf(data):\n",
    "    tf_idf = TF_IDF.transform(data)\n",
    "    return tf_idf.toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = extract_bags_of_words(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf = extract_tf_idf(train_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the bag of words for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow = extract_bags_of_words(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the TF-IDF for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tf_idf = extract_tf_idf(dev_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# train_features = np.concatenate((train_bow, train_tf_idf), axis=1)\n",
    "train_features = train_bow\n",
    "# test_features = np.concatenate((test_bow, test_tf_idf), axis=1)\n",
    "test_features = test_bow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(train_features, train_stances.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = naive_bayes.MultinomialNB()\n",
    "nb_clf.fit(train_features, train_stances.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = svm_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.56      0.07      0.13        70\n",
      "           0       0.48      0.08      0.14       126\n",
      "           1       0.82      0.99      0.90       804\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.62      0.38      0.39      1000\n",
      "weighted avg       0.76      0.81      0.75      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(dev_stances, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3866812300343862\n"
     ]
    }
   ],
   "source": [
    "svm_predictions_f1 = f1_score(dev_stances, svm_predictions, average='macro')\n",
    "print(svm_predictions_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
