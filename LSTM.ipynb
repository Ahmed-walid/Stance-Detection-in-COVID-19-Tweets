{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4c03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Imports and Data Loading ========= #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea5ba47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "303a761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing():\n",
    "    def __init__(self, train_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_df = self.load_data(train_path)\n",
    "        self.value_counts = self.train_df['stance'].value_counts()\n",
    "        \n",
    "        self.train_set = list(self.train_df.to_records(index=False))\n",
    "        self.tokenizate()\n",
    "        \n",
    "    def load_data(self, train_path):\n",
    "        return pd.read_csv(train_path)\n",
    "    \n",
    "    def remove_links_mentions(self, tweet):\n",
    "        link_re_pattern = \"https?:\\/\\/t.co/[\\w]+\"\n",
    "        mention_re_pattern = \"@\\w+\"\n",
    "        tweet = re.sub(link_re_pattern, \"\", tweet)\n",
    "        tweet = re.sub(mention_re_pattern, \"\", tweet)\n",
    "        return tweet.lower()\n",
    "    \n",
    "    def tokenizate(self):\n",
    "        self.train_set = [(category, label, word_tokenize(self.remove_links_mentions(tweet))) for (tweet, category, label) in self.train_set]\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        return self.train_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6e51fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsDataset(Dataset):\n",
    "    def __init__(self, train_set):\n",
    "        super().__init__()\n",
    "        self.train_set = train_set\n",
    "                \n",
    "        self.index2word = self.construct_index2word()\n",
    "        self.voc_size = len(self.index2word)\n",
    "        self.word2index = {token: idx for idx, token in enumerate(self.index2word)}\n",
    "\n",
    "        self.max_seq_length = self.get_max_seq()\n",
    "        self.train_encoded = [(category, label, self.encode_and_pad(tweet, self.max_seq_length)) for category, label, tweet in self.train_set]\n",
    "        \n",
    "        self.data = (np.array([tweet for category, label, tweet in self.train_encoded]))\n",
    "        self.label = (np.array([label for category, label, tweet in self.train_encoded]))\n",
    "\n",
    "        self.train_ds = TensorDataset(torch.from_numpy(self.data), torch.from_numpy(self.label))\n",
    "        \n",
    "#         Add Categories in train_ds\n",
    "        \n",
    "    def construct_index2word(self):\n",
    "        index2word = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "        for ds in [self.train_set]:\n",
    "            for category, label, tweet in ds:\n",
    "                for token in tweet:\n",
    "                    if token not in index2word:\n",
    "                        index2word.append(token)\n",
    "        return index2word\n",
    "    \n",
    "    def get_max_seq(self):\n",
    "        max_seq_length = 0\n",
    "        for tweet, category, label in self.train_set:\n",
    "            if len(tweet) > max_seq_length:\n",
    "                max_seq_length = len(tweet)\n",
    "        return max_seq_length\n",
    "    \n",
    "    def encode_and_pad(self, tweet, length):\n",
    "        sos = [self.word2index[\"<SOS>\"]]\n",
    "        eos = [self.word2index[\"<EOS>\"]]\n",
    "        pad = [self.word2index[\"<PAD>\"]]\n",
    "\n",
    "        if len(tweet) < length - 2: # -2 for SOS and EOS\n",
    "            n_pads = length - 2 - len(tweet)\n",
    "            encoded = [self.word2index[w] for w in tweet]\n",
    "            return sos + encoded + eos + pad * n_pads \n",
    "        else: # tweet is longer than possible; truncating\n",
    "            encoded = [self.word2index[w] for w in tweet]\n",
    "            truncated = encoded[:length - 2]\n",
    "            return sos + truncated + eos\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c62605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding_size = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = self.embedding(torch.tensor(input))\n",
    "        out, _ = self.rnn(input)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "780c68da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnTweetsClassifier():\n",
    "    def __init__(self, dataset, embedding_dim, hidden_size, num_layers, num_classes, epoch_size, learning_rate):\n",
    "        self.dataset = dataset\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.epoch_size = epoch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = RNN(dataset.voc_size, embedding_dim, hidden_size, num_layers, num_classes)\n",
    "        self.data_loader = DataLoader(dataset=dataset.get_data(), batch_size=256, shuffle=True)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), self.learning_rate)\n",
    "         \n",
    "    def train(self):\n",
    "        for epoch in range(self.epoch_size):\n",
    "            total_model_acc = 0\n",
    "            for data in self.data_loader:\n",
    "                train_data = data[:][0]\n",
    "                labels = data[:][1]\n",
    "                # reshape the row data\n",
    "                output = self.model(train_data)\n",
    "                # calculate the accuracy\n",
    "                total_model_acc += (torch.argmax(output, dim=1) == (labels + 1)).sum()\n",
    "                loss = self.criterion(output, (labels + 1))\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_model_acc = total_model_acc / len(self.dataset.get_data())\n",
    "            print(f\"epoch num: {epoch} has accuracy: {total_model_acc}\")\n",
    "\n",
    "    def predict(self, testLoader):\n",
    "        with torch.no_grad():\n",
    "            n_correct = 0\n",
    "            n_samples = 0\n",
    "            for tweets in testLoader:\n",
    "                test_data = tweets[:][0]\n",
    "                labels = tweets[:][1]\n",
    "                output = self.model(test_data)\n",
    "                n_correct += (torch.argmax(output, dim=1) == (labels + 1)).sum()\n",
    "                n_samples += labels.size(0)\n",
    "            print(f\"Model accuracy: {100 * n_correct / n_samples}\")\n",
    "            print(\"Model F1 Score: \",f1_score(labels, torch.argmax(output,dim=1), average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "536a7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMP\\AppData\\Local\\Temp/ipykernel_3584/1089490090.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = self.embedding(torch.tensor(input))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch num: 0 has accuracy: 0.7734687924385071\n",
      "epoch num: 1 has accuracy: 0.7925014495849609\n",
      "epoch num: 2 has accuracy: 0.7925014495849609\n",
      "epoch num: 3 has accuracy: 0.7925014495849609\n",
      "epoch num: 4 has accuracy: 0.7935031652450562\n",
      "epoch num: 5 has accuracy: 0.7993703484535217\n",
      "epoch num: 6 has accuracy: 0.8112478256225586\n"
     ]
    }
   ],
   "source": [
    "# =========== Training =========== #\n",
    "train_path = \"./dataset/train.csv\"\n",
    "preprocessing = Preprocessing(train_path)\n",
    "train = preprocessing.get_datasets()\n",
    "\n",
    "dataset = TweetsDataset(train)\n",
    "tweet_classifier = RnnTweetsClassifier(dataset, 50, 50, 1, 3, 7, 0.001)\n",
    "tweet_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e427583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 71.19999694824219\n",
      "Model F1 Score:  0.07432432432432433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMP\\AppData\\Local\\Temp/ipykernel_3584/1089490090.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = self.embedding(torch.tensor(input))\n"
     ]
    }
   ],
   "source": [
    "# =========== Test =========== #\n",
    "dev_path = \"./dataset/dev.csv\"\n",
    "preprocessing = Preprocessing(dev_path)\n",
    "dev = preprocessing.get_datasets()\n",
    "testset = TweetsDataset(dev)\n",
    "testloader = DataLoader(dataset=testset.get_data(), batch_size=256, shuffle=True)\n",
    "tweet_classifier.predict(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316abad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
